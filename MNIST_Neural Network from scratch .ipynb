{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST contains 70000 images of hand-written digits\n",
    "each 28 * 28 pixels in gryscale with pixel-values from 0 to 255. We could download and preprocess the data ourseleves. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml \n",
    "X,y  = fetch_openml('mnist_784', version=1, \n",
    "                          return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '0' '4' '1' '9' '2' '1' '3' '1' '4' '3' '5' '3' '6']\n",
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "#to normalize the X values to keep it under 0 to 255 \n",
    "X= X/255\n",
    "print(y[:14])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_new =np.zeros(y.shape)\n",
    "for i in range(y.shape[0]):\n",
    "    if(y[i]=='0'):\n",
    "        y_new[i]=1\n",
    "print(y_new)\n",
    "y=y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset into training and testing\n",
      "training samples :60000 test samples:10000\n",
      "train X shape: 60000\n",
      "test X shape:10000\n",
      "train y shape: (1, 60000)\n",
      "test y shape:(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# y contains 1 for ifthe image is 0 or else 0 \n",
    "#for non-zero classes\n",
    "train_length = 60000\n",
    "test_length = X.shape[0]-train_length\n",
    "print(\"Splitting the dataset into training and testing\")\n",
    "print(\"training samples :\"+str(train_length)+\" test samples:\"+str(test_length))\n",
    "X_train , X_test = X[:train_length].T,X[train_length:].T\n",
    "print(\"train X shape: \"+str(X_train.shape[1])+\"\\ntest X shape:\"+str(X_test.shape[1]))\n",
    "y_train,y_test = y[:train_length].reshape(1,train_length),y[train_length:].reshape(1,test_length)\n",
    "print(\"train y shape: \"+str(y_train.shape)+\"\\ntest y shape:\"+str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(138)\n",
    "shuffle_index = np.random.permutation(train_length)\n",
    "X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing data samples from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\t[1.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t[0.]\t"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i=3 \n",
    "nrows= 5\n",
    "ncols=4\n",
    "for i in range(20):\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('on') # Don't show axes (or gridlines)\n",
    "    plt.imshow(X_train[:,i].reshape(28,28),cmap = matplotlib.cm.binary)\n",
    "    print(y_train[:,i],end=\"\\t\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Single neuron (aka logistic regression )\n",
    "#Single class classification\n",
    "As we are considering only '0' class the target variable is either 0 if it is not zero in image or 1 if is a zero in image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation: \n",
    "y = sigma(w^T * x +b)\n",
    "signmoid fuction: \n",
    "sigma(z) = (1 / (1+e^-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the s here is the Activation function value\n",
    "def sigmoid(z): \n",
    "    s= 1/(1 + np.exp(-z))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z value for the sigmoid function can be computed by \n",
    "\n",
    "np.matmul(W.T, X)+b \n",
    "\n",
    "hence z value is not a scalar but a vector\n",
    "We’ll use cross-entropy for our cost function. \n",
    "The formula for a single training example is: \n",
    "\n",
    "L(y,ŷ)=−ylog(ŷ)−(1−y)log(1−ŷ)\n",
    "\n",
    "Averaging over a training set of m examples we then have:\n",
    "\n",
    "L(Y,Ŷ)=−1/m∑i=1m(y^(i)log(ŷ^(i))+(1−y^(i))log(1−ŷ^(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y,Y_hat):\n",
    "    m= Y.shape[1]\n",
    "    L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can substitute into the chain rule to find:\n",
    "∂L/∂wj=∂L/∂ŷ*∂ŷ/∂z*∂z/∂wj  ---> (ŷ−y)wj [Scalar quantity]\n",
    "\n",
    "1)∂L/∂w=1/m*X(ŷ−y)^T --->[Vector quantity]\n",
    "2)∂L/∂b=1/m*∑i=1Tom(ŷ(i)−y(i)) --->[Vector quantity]\n",
    "\n",
    "i’ll label these gradients according to their denominators, as dW and db. So for backpropagation we’ll compute \n",
    "\n",
    "dW = (1/m) * np.matmul(X, (A-Y).T) -->[∂L/∂w]and \n",
    "\n",
    "db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)[∂L/∂w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 60000 \n",
      "n_x(first layer) 60000 \n",
      "Number of samples:  784\n",
      "bias: [[0.]]\n",
      "Epoch 0 cost:  0.690062639294071\n",
      "Epoch 100 cost:  0.041353631903225475\n",
      "Epoch 200 cost:  0.03582155605411296\n",
      "Epoch 300 cost:  0.03325263912475694\n",
      "Epoch 400 cost:  0.03164906103353909\n",
      "Epoch 500 cost:  0.030507386195851624\n",
      "Epoch 600 cost:  0.029633944733120827\n",
      "Epoch 700 cost:  0.028935287004216412\n",
      "Epoch 800 cost:  0.02835929538826425\n",
      "Epoch 900 cost:  0.027873871059235496\n",
      "Epoch 1000 cost:  0.027457797874105336\n",
      "Epoch 1100 cost:  0.027096307087125685\n",
      "Epoch 1200 cost:  0.026778707482164456\n",
      "Epoch 1300 cost:  0.026497019916701066\n",
      "Epoch 1400 cost:  0.02624514201879309\n",
      "Epoch 1500 cost:  0.026018312084556613\n",
      "Epoch 1600 cost:  0.025812751368771314\n",
      "Epoch 1700 cost:  0.02562541766330368\n",
      "Epoch 1800 cost:  0.02545383095639554\n",
      "Epoch 1900 cost:  0.025295947270018212\n",
      "Final cost: 0.02515146986256394\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "X=X_train\n",
    "y=y_train\n",
    "\n",
    "n_x = X.shape[0]\n",
    "m=X.shape[1]\n",
    "print(X.shape[1],y.shape[1],\"\\nn_x(first layer)\",m\n",
    "      ,\"\\nNumber of samples: \",n_x)\n",
    "W=np.random.randn(n_x,1)*0.01\n",
    "#fill zeros for (rows, cols) -> (1,1) 1row & 1col\n",
    "b=np.zeros((1,1))\n",
    "print(\"bias: \"+str(b))\n",
    "\n",
    "for i in range(2000):\n",
    "    Z=np.matmul(W.T,X)+b\n",
    "    A=sigmoid(Z)\n",
    "    # y_hat = Activation function\n",
    "    cost = compute_loss(y,A)\n",
    "    \n",
    "    dW = (1/m) * np.matmul(X, (A-y).T) \n",
    "    db = (1/m) * np.sum(A-y, axis=1, keepdims=True)\n",
    "    \n",
    "    W = W -learning_rate * dW\n",
    "    b = b -learning_rate * db\n",
    "    \n",
    "    if(i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \",cost)\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8979   33]\n",
      " [  41  947]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      9012\n",
      "        True       0.97      0.96      0.96       988\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "Z = np.matmul(W.T, X_test) + b\n",
    "A = sigmoid(Z)\n",
    "\n",
    "predictions = (A>.5)[0,:]\n",
    "labels = (y_test == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one layer + hidden layer(64 neurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (1, 60000) \n",
      "n_x(first layer) 60000 \n",
      "Number of samples:  784 \n",
      "Number of hidden neurons: 64\n",
      "Weight1 W1:  (64, 784) \tbias1 b1:  (64, 1) \n",
      "Weight2 W2:  (1, 64) \tbias2 b2 (1, 1)\n",
      "Epoch 0 cost:  1.8781126759697913\n",
      "Epoch 100 cost:  0.09709508030746875\n",
      "Epoch 200 cost:  0.0695532253508136\n",
      "Epoch 300 cost:  0.05814555211646256\n",
      "Epoch 400 cost:  0.051440027421343244\n",
      "Epoch 500 cost:  0.04686013556977249\n",
      "Epoch 600 cost:  0.04346121711728291\n",
      "Epoch 700 cost:  0.04079397593660481\n",
      "Epoch 800 cost:  0.038614945123903906\n",
      "Epoch 900 cost:  0.036785445049844594\n",
      "Epoch 1000 cost:  0.03522227753067303\n",
      "Epoch 1100 cost:  0.03386775964008793\n",
      "Epoch 1200 cost:  0.03267689896107443\n",
      "Epoch 1300 cost:  0.031615142577526664\n",
      "Epoch 1400 cost:  0.030657001499035802\n",
      "Epoch 1500 cost:  0.029783958950155377\n",
      "Epoch 1600 cost:  0.02898258083071087\n",
      "Epoch 1700 cost:  0.02824305224473213\n",
      "Epoch 1800 cost:  0.027557921045011192\n",
      "Epoch 1900 cost:  0.026920985102467678\n",
      "Final cost: 0.026332407319913483\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.8\n",
    "X=X_train\n",
    "y=y_train\n",
    "\n",
    "#number of neurons in the first layer\n",
    "n_x = X.shape[0]\n",
    "m=X.shape[1]\n",
    "#number of neurons in the hidden layer\n",
    "n_h=64\n",
    "\n",
    "print(X.shape,y.shape,\"\\nn_x(first layer)\",m\n",
    "      ,\"\\nNumber of samples: \",n_x,\"\\nNumber of hidden neurons:\",n_h)\n",
    "#randn method generates the random values in the form \n",
    "#of matrix . In the below case, as the weights go between the \n",
    "#fully connected layer(60000 samples) and hidden layer (64 neurons)\n",
    "W1=np.random.randn(n_h,n_x)\n",
    "#fill zeros for (rows, cols) -> (1,1) 1row & 1col\n",
    "#in this case (64,1) 64 rows and 1 col\n",
    "b1=np.zeros((n_h,1))\n",
    "#In this case the randn function one row with 64 columns \n",
    "W2 =np.random.randn(1,n_h)\n",
    "#in this case the output of the model should be 1 value \n",
    "# so the output is the 1 row and 1 column\n",
    "b2=np.zeros((1,1))\n",
    "\n",
    "print(\"Weight1 W1: \",W1.shape,\"\\tbias1 b1: \",b1.shape,\"\\nWeight2 W2: \",W2.shape,\"\\tbias2 b2\",b2.shape)\n",
    "\n",
    "for i in range(2000):\n",
    "    Z1=np.matmul(W1,X)+b1\n",
    "    A1=sigmoid(Z1)\n",
    "    Z2=np.matmul(W2,A1)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "    # y_hat = Activation function\n",
    "    cost = compute_loss(y,A2)\n",
    "\n",
    "    dZ2 = A2-y\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "          \n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8980   40]\n",
      " [  40  940]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      9020\n",
      "        True       0.96      0.96      0.96       980\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "predictions = (A2>.5)[0,:]\n",
    "labels = (y_test == 1)[0,:]\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "X,y  = fetch_openml('mnist_784', version=1, \n",
    "                          return_X_y=True)\n",
    "X=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "digits = 10\n",
    "examples= y.shape[0]\n",
    "print(y.shape)\n",
    "\n",
    "y = y.reshape(1,examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_new shape: (1, 70000, 10)\n",
      "[[5 0 4 ... 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#np.eye returns an array of size of digits i.e 10(in this case) \n",
    "#y_new has 1's in the diagonal and 0's elsewhere\n",
    "#y.astype('int32') argument is used to put 1's in the position\n",
    "#according to the y array number in such a way to 70000 samples \n",
    "# each having 1 in the position of the digit depiction\n",
    "y_new = np.eye(digits)[y.astype('int32')]\n",
    "print(\"y_new shape:\",y_new.shape)\n",
    "print(y.astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_new: shape  (10, 70000)\n"
     ]
    }
   ],
   "source": [
    "y_new = y_new.T.reshape(digits,examples)\n",
    "print(\"y_new: shape \",y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-split , rehape and re-shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_of_samples_train = 60000\n",
    "No_of_samples_test = X.shape[0]-m\n",
    "\n",
    "X_train, X_test = X[:m].T,X[m:].T\n",
    "Y_train,Y_test = y_new[:,:m],y_new[:,m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train,Y_train = X_train[:,shuffle_index],Y_train[:,shuffle_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAGM0lEQVR4nO3dz4uN/R/H8TO6IzSTGqaGRLbDahZKyMYKNdmwsbOh7KwprKSkZKGR9WSlyEKsCBuzIcXOlJXdlI0f5/4HzvW+vt9znHteF4/H0qtr5ty3nvdV96frXBP9fr8H5Fm31h8AGEycEEqcEEqcEEqcEOqflt3/yoXxmxj0h+6cEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEOqftf4A8L/48OFDuc/NzZX76dOny/3OnTvlvmXLlnIfB3dOCCVOCCVOCCVOCCVOCCVOCCVOCOWckxiPHj1q3C5evFheOzExUe6fP38u9w0bNpT7WnDnhFDihFDihFDihFDihFDihFAT/X6/2svxb3Xr1q1yX1paKve7d+82bnv37h3qM3XByspKuR87dqxxe/fuXXnttm3byv3Bgwflfvjw4XIfs4HnQO6cEEqcEEqcEEqcEEqcEEqcEEqcEMojY0OYnp4u9zdv3pT7zZs3G7d79+4N9Zm64OzZs+X+/v37xm379u3ltW/fvi33mZmZck/kzgmhxAmhxAmhxAmhxAmhxAmhxAmhnHMOsLq6Wu7VOWWv1+u1PCPbm5+f/78/U4IfP36U+7Vr18r96dOn5T45Odm4PXz4sLy2i+eYbdw5IZQ4IZQ4IZQ4IZQ4IZQ4IZQ4IZRzzgEWFxfLfXl5udynpqbKfY2/I3Vobc9MXr16tdzbXtN36dKlxq2rZ8OjcOeEUOKEUOKEUOKEUOKEUOKEUOKEUH/lOeezZ8/K/f79+yP9/JMnT5Z78js4P3782LgtLCyM9LPPnTtX7hcuXBjp5/9p3DkhlDghlDghlDghlDghlDgh1ETL1zjW3/HYUQcOHCj3tlf4zc7OlvunT5/KfePGjeU+TtVRSa/X6x09erRxW1lZKa/dsWNHub948aLcd+3aVe5/sIHP0rlzQihxQihxQihxQihxQihxQihxQqjOPjLW9jq6x48fN26vX78e6Xf/+vWr3L9+/VrumzZtatymp6fLa9teT7h+/fpyv3HjRrlXZ5ltrzY8c+ZMuf/F55hDceeEUOKEUOKEUOKEUOKEUOKEUOKEUJ19nrPt2cLdu3eP7Xe3nfe1vepu586djdv+/fvLa9uex9y6dWu5P3/+vNwrbf/cbc/Jtj3vWTl06FC5HzlypNyTv46053lO6BZxQihxQihxQihxQihxQihxQijnnEMY9ZzT7/79v7vt7/vly5fl3vZdxGPmnBO6RJwQSpwQSpwQSpwQSpwQSpwQqrPfWzs5OVnup06datyWl5fLa48fP17umzdvLve2M7P5+fnG7cuXL+W1CwsL5T6q6rO1PVO5bt34/lvf9u/8/Pnz5T4zM/M7P85/wp0TQokTQokTQokTQokTQokTQnX2kbE/Vdsr/qampsq97bGtgwcPlnv16sS24yuG5pEx6BJxQihxQihxQihxQihxQihxQqjOPjL2p1pcXCz3tnPMtv369evl7iwzhzsnhBInhBInhBInhBInhBInhBInhHLOGWZpaWmk6/ft21fuc3NzI/18/jvunBBKnBBKnBBKnBBKnBBKnBBKnBDKOecauHLlSuPW9nrCtuctb9++PdL15HDnhFDihFDihFDihFDihFDihFCOUtbAkydPGrfv37+X1+7Zs6fc217xR3e4c0IocUIocUIocUIocUIocUIocUIo55xjsLq6Wu7fvn1r3Npe4Xf58uWhPhPd484JocQJocQJocQJocQJocQJocQJoSb6/X61lyODvXr1qtxHeeby58+fQ19LrIGH2+6cEEqcEEqcEEqcEEqcEEqcEEqcEMrznGMwOzs79H7ixInf/XHoKHdOCCVOCCVOCCVOCCVOCCVOCCVOCOV5Tlh7nueELhEnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhGp7BeDAr+wDxs+dE0KJE0KJE0KJE0KJE0KJE0L9C9Pf8ZNU9IboAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "Y_train[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation: \n",
    "The final output which is one node will be replaced by 10 nodes \n",
    "because of representation of 10 digits\n",
    "Hence the activation function sigma(z) is calculated for each unit i :e^zi/∑(j=0 to 9)e^zj\n",
    "\n",
    "\n",
    "Cost funtion : has to generalize to more than 2 classes. The general formula for n classes is \n",
    "L(y,ŷ)=−∑(i=0 to n)y(i)*log(ŷi).\n",
    "Averaging over m samples: \n",
    "L(Y,Ŷ)=−1/m∑(j=0 to m)∑(i=0 to n)y^(j)log(ŷ^(j)).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiclass_loss(Y,Y_hat):\n",
    "    \n",
    "    L_sum = np.sum(np.multiply(Y,np.log(Y_hat)))\n",
    "    m=Y.shape[1]\n",
    "    L=-(1/m)* L_sum\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  7.376709570967475\n",
      "Epoch 100 cost:  0.7428469430716562\n",
      "Epoch 200 cost:  0.5557586080703597\n",
      "Epoch 300 cost:  0.4776808468365544\n",
      "Epoch 400 cost:  0.43157507082224356\n",
      "Epoch 500 cost:  0.39986280333244556\n",
      "Epoch 600 cost:  0.37602413331009715\n",
      "Epoch 700 cost:  0.357049495270229\n",
      "Epoch 800 cost:  0.3413467382449301\n",
      "Epoch 900 cost:  0.32796809522028375\n",
      "Epoch 1000 cost:  0.31631003965939253\n",
      "Epoch 1100 cost:  0.30597591202287994\n",
      "Epoch 1200 cost:  0.2966956655821704\n",
      "Epoch 1300 cost:  0.2882768356274176\n",
      "Epoch 1400 cost:  0.2805759948619913\n",
      "Epoch 1500 cost:  0.27348394303519713\n",
      "Epoch 1600 cost:  0.2669165492486281\n",
      "Epoch 1700 cost:  0.2608069381940334\n",
      "Epoch 1800 cost:  0.255099277801469\n",
      "Epoch 1900 cost:  0.24974528614366043\n",
      "Final cost: 0.2447520558080259\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "learning_rate = 1\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(digits, n_h)\n",
    "b2 = np.zeros((digits, 1))\n",
    "\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    Z1 = np.matmul(W1,X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(W2,A1) + b2\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "\n",
    "    cost = compute_multiclass_loss(Y, A2)\n",
    "\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 952    0   15    1    1   12   15    1    7    4]\n",
      " [   0 1102    2    2    0    2    3    9    9    4]\n",
      " [   5    4  924   22   10    3    7   26   13    2]\n",
      " [   1    6   20  918    2   33    3   10   26   16]\n",
      " [   1    0   13    0  906    9   18   11    8   43]\n",
      " [   8    4   11   26    2  773   14    6   28    6]\n",
      " [   7    5    6    3    9   16  891    0    6    4]\n",
      " [   3    1   14    8    8   12    3  930   12   23]\n",
      " [   3   13   24   24    6   25    3    4  858    9]\n",
      " [   0    0    3    6   38    7    1   31    7  898]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96      1008\n",
      "           1       0.97      0.97      0.97      1133\n",
      "           2       0.90      0.91      0.90      1016\n",
      "           3       0.91      0.89      0.90      1035\n",
      "           4       0.92      0.90      0.91      1009\n",
      "           5       0.87      0.88      0.87       878\n",
      "           6       0.93      0.94      0.94       947\n",
      "           7       0.90      0.92      0.91      1014\n",
      "           8       0.88      0.89      0.88       969\n",
      "           9       0.89      0.91      0.90       991\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "\n",
    "predictions = np.argmax(A2, axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(confusion_matrix(predictions, labels))\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optmized model for boosting accuracy- \n",
    "The difference b/w previous and current model is that Mini batch descent is considered by adding a another for loop inside each batch processing for loop. At each pass, the training set is shuffled and batch size is 128. \n",
    "setting the variance of the initialization to 1/n shrinks the weights. Hence while initializing weights dividing by  p.sqrt(1./n_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# import\n",
    "from sklearn.datasets import fetch_openml \n",
    "X,y  = fetch_openml('mnist_784', version=1, \n",
    "                          return_X_y=True)\n",
    "\n",
    "# scale\n",
    "X = X / 255\n",
    "\n",
    "# one-hot encode labels\n",
    "digits = 10\n",
    "examples = y.shape[0]\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "# split, reshape, shuffle\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def compute_loss(Y, Y_hat):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L\n",
    "\n",
    "def feed_forward(X, params):\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache\n",
    "\n",
    "def back_propagate(X, Y, params, cache):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training cost = 0.15289188610758328, test cost = 0.15896132708794716\n",
      "Epoch 2: training cost = 0.10085958305622832, test cost = 0.11929251693876221\n",
      "Epoch 3: training cost = 0.09060707621694518, test cost = 0.12329525126099913\n",
      "Epoch 4: training cost = 0.06342341756474529, test cost = 0.09910809479271464\n",
      "Epoch 5: training cost = 0.05982493064697334, test cost = 0.09451228374481334\n",
      "Epoch 6: training cost = 0.04747558007215102, test cost = 0.09014035845674313\n",
      "Epoch 7: training cost = 0.046833023412061935, test cost = 0.09595219893815515\n",
      "Epoch 8: training cost = 0.03534418411103081, test cost = 0.09147580432857733\n",
      "Epoch 9: training cost = 0.03175458971480449, test cost = 0.08557197998648979\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(138)\n",
    "\n",
    "# hyperparameters\n",
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "learning_rate = 4\n",
    "beta = .9\n",
    "batch_size = 128\n",
    "batches = -(-m // batch_size)\n",
    "\n",
    "# initialization\n",
    "params = { \"W1\": np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((n_h, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(digits, n_h) * np.sqrt(1. / n_h),\n",
    "           \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / n_h) }\n",
    "\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "\n",
    "# train\n",
    "for i in range(9):\n",
    "\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "        params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_cost = compute_loss(Y_train, cache[\"A2\"])\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_cost = compute_loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training cost = {}, test cost = {}\".format(i+1 ,train_cost, test_cost))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       984\n",
      "           1       0.99      0.99      0.99      1136\n",
      "           2       0.97      0.97      0.97      1036\n",
      "           3       0.99      0.96      0.97      1040\n",
      "           4       0.97      0.98      0.97       975\n",
      "           5       0.96      0.98      0.97       876\n",
      "           6       0.97      0.98      0.98       955\n",
      "           7       0.96      0.98      0.97      1005\n",
      "           8       0.97      0.96      0.97       985\n",
      "           9       0.97      0.97      0.97      1008\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache = feed_forward(X_test, params)\n",
    "predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Accuracy is 97% of the model with one fully connected layer, one hidden layer and softmax with mini batch gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
